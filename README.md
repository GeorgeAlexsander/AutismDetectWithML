# ğŸ¯ Projeto: DetecÃ§Ã£o Precoce de Autismo com Machine Learning

Este projeto visa desenvolver um sistema baseado em **aprendizado de mÃ¡quina** para a detecÃ§Ã£o precoce do autismo por meio da anÃ¡lise de **imagens faciais**. O sistema utiliza **mediÃ§Ãµes antropomÃ©tricas** obtidas a partir de **landmarks faciais**, extraÃ­dos atravÃ©s de tÃ©cnicas de processamento de imagem. A estrutura do projeto estÃ¡ organizada conforme descrito abaixo.

## ğŸ“‚ Estrutura do Projeto

```bash
project_root/
â”‚
â”œâ”€â”€ data/                          # DiretÃ³rio para armazenar dados
â”‚   â”œâ”€â”€ raw/                       # Dados brutos (nÃ£o processados)
â”‚   â”‚   â”œâ”€â”€ autism_dataset/        # Conjunto de dados sobre autismo
â”‚   â”‚   â”‚   â”œâ”€â”€ no_autism/         # Dados de indivÃ­duos sem autismo
â”‚   â”‚   â”‚   â”œâ”€â”€ with_autism/       # Dados de indivÃ­duos com autismo
â”‚   â”‚   â”‚   â””â”€â”€ ...                # Outros subdiretÃ³rios conforme necessÃ¡rio
â”‚   â”‚   â””â”€â”€ processed/             # Dados processados (prontos para uso)
â”‚   â”‚       â””â”€â”€ ...                # Estrutura dos dados processados
â”‚   â””â”€â”€ README.md                  # DocumentaÃ§Ã£o sobre os dados
â”‚
â”œâ”€â”€ notebooks/                     # Notebooks Jupyter para anÃ¡lise
â”‚   â””â”€â”€ exploratory_analysis.ipynb # AnÃ¡lise exploratÃ³ria dos dados
â”‚
â”œâ”€â”€ src/                           # CÃ³digo-fonte do projeto
â”‚   â”œâ”€â”€ __init__.py                # Torna o diretÃ³rio um pacote Python
â”‚   â”œâ”€â”€ data_processing.py         # Processamento de dados brutos
â”‚   â”œâ”€â”€ feature_extraction.py      # ExtraÃ§Ã£o de caracterÃ­sticas
â”‚   â”œâ”€â”€ model_training.py          # Treinamento do modelo
â”‚   â”œâ”€â”€ model_evaluation.py        # AvaliaÃ§Ã£o do modelo
â”‚   â””â”€â”€ utils.py                   # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ tests/                         # Testes automatizados
â”‚   â”œâ”€â”€ __init__.py                # Torna o diretÃ³rio um pacote Python
â”‚   â”œâ”€â”€ test_data_processing.py    # Testes de processamento de dados
â”‚   â”œâ”€â”€ test_feature_extraction.py # Testes de extraÃ§Ã£o de caracterÃ­sticas
â”‚   â””â”€â”€ ...                        # Outros testes conforme necessÃ¡rio
â”‚
â”œâ”€â”€ docs/                          # DocumentaÃ§Ã£o do projeto
â”‚   â”œâ”€â”€ index.md                   # Ãndice da documentaÃ§Ã£o
â”‚   â””â”€â”€ usage.md                   # Como usar o projeto
â”‚
â”œâ”€â”€ .gitignore                     # Arquivos a serem ignorados pelo Git
â”œâ”€â”€ README.md                      # DocumentaÃ§Ã£o principal do projeto
â”œâ”€â”€ requirements.txt               # DependÃªncias do projeto
â””â”€â”€ setup.py                       # ConfiguraÃ§Ã£o do pacote
```

## ğŸ§  Topologia do Modelo de Machine Learning

## âš™ï¸ Estrutura do Modelo

O modelo de detecÃ§Ã£o de autismo em imagens faciais Ã© composto por trÃªs etapas principais:

### ğŸ§© Etapa 1: ExtraÃ§Ã£o de Landmark Facial

A primeira etapa utiliza uma **CNN prÃ©-treinada** para detectar landmarks faciais. SerÃ£o exploradas trÃªs abordagens:

- **Haarcascade com OpenCV (cv2)**: Detecta landmarks faciais com **68 pontos de referÃªncia**.
- **dlib**: Detecta landmarks faciais com **68 pontos de referÃªncia**, usando o dlib.
- **MediaPipe (Google)**: Utiliza um modelo de **malha facial 3D** com **438 pontos de referÃªncia**.

ğŸ”¹ **A saÃ­da serÃ¡ um conjunto de landmarks faciais**, com 68 pontos para Haarcascade e dlib, e 438 pontos para MediaPipe.

### ğŸ§® Etapa 2: CÃ¡lculo de MediÃ§Ãµes AntropomÃ©tricas

Com os landmarks extraÃ­dos, sÃ£o calculadas **8 mediÃ§Ãµes antropomÃ©tricas** usando a fÃ³rmula da **DistÃ¢ncia Euclidiana**. Os resultados sÃ£o armazenados em um arquivo CSV.

ğŸ“Š **MediÃ§Ãµes**:

| ReferÃªncia 1           | ReferÃªncia 2           | DistÃ¢ncia  | DefiniÃ§Ã£o                |
|------------------------|------------------------|------------|--------------------------|
| Trichion (tr)           | Glabella (gl)          | Vertical   | Altura facial superior    |
| Glabella (gl)           | Filtrum superior (pu)  | Vertical   | Altura facial mÃ©dia       |
| Parte superior (pu)     | Menton (me)            | Vertical   | Altura facial inferior    |
| Filtrum superior (pu)   | Filtrum inferior (pl)  | Vertical   | Filtrum                  |
| Endo canthus (enl)      | Endo canthus (enr)     | Horizontal | Largura intercantal       |
| Exo canthus (exl)       | Exo canthus (exr)      | Horizontal | Largura biocular          |
| Alare (all)             | Alare (alr)            | Horizontal | Largura nasal             |
| Cheilion (chr)          | Cheilion (chl)         | Horizontal | Largura da boca           |

### ğŸ“Š Etapa 3: ClassificaÃ§Ã£o com Redes Neurais e Algoritmos de Machine Learning

Os dados extraÃ­dos (CSV com mediÃ§Ãµes antropomÃ©tricas) serÃ£o utilizados para treinar um modelo de classificaÃ§Ã£o. SerÃ£o experimentados trÃªs algoritmos:

- **CNN (Redes Neurais Convolutivas)**: Para classificaÃ§Ã£o baseada em mediÃ§Ãµes antropomÃ©tricas.
- **K-Nearest Neighbors (KNN)**: Para comparaÃ§Ã£o de precisÃ£o.
- **Random Forest**: Para comparaÃ§Ã£o de desempenho e precisÃ£o.

ğŸ“ **O objetivo Ã© prever se a crianÃ§a apresenta sinais de autismo.**

## ğŸ“‹ PadronizaÃ§Ã£o do RepositÃ³rio e Ferramentas Utilizadas

### ğŸ—‚ï¸ Estrutura e ConfiguraÃ§Ã£o do RepositÃ³rio

A estrutura do repositÃ³rio segue um padrÃ£o para facilitar a organizaÃ§Ã£o e manutenÃ§Ã£o do projeto. Ã‰ utilizado o sistema de controle de versÃ£o Git com o modelo de branches Git Flow.

ğŸ”§ **OrganizaÃ§Ã£o do repositÃ³rio** inclui as pastas descritas acima, com detalhes a seguir:

- **data/**: Armazena dados brutos e processados.
- **notebooks/**: Armazena Jupyter Notebooks para anÃ¡lise exploratÃ³ria.
- **src/**: ContÃ©m o cÃ³digo-fonte do projeto, dividido em mÃ³dulos.
- **tests/**: Armazena testes automatizados.
- **docs/**: DocumentaÃ§Ã£o do projeto.

### ğŸ› ï¸ Controle de VersÃ£o e ColaboraÃ§Ã£o

Utilize branches para funcionalidades (**feature/**), correÃ§Ãµes (**bugfix/**), e versÃµes (**release/**), garantindo organizaÃ§Ã£o e rastreabilidade.

### ğŸ’» ConfiguraÃ§Ã£o do Ambiente de Desenvolvimento

O ambiente de desenvolvimento utiliza o **Visual Studio Code** com extensÃµes para **Python**, **Git**, e **Docker**. Ferramentas adicionais incluem:

- **Pylint**: Para linting.
- **Black**: Para formataÃ§Ã£o automÃ¡tica de cÃ³digo.
- **Jupyter Notebooks**: Para experimentaÃ§Ã£o e anÃ¡lise de dados.

### ğŸ”§ Linguagem de ProgramaÃ§Ã£o e Bibliotecas

O sistema Ã© desenvolvido em **Python**, utilizando bibliotecas como:

- **TensorFlow** e **Keras**: Para desenvolvimento e treinamento do modelo.
- **OpenCV**: Para processamento de imagens.
- **Pandas** e **NumPy**: Para manipulaÃ§Ã£o de dados.

### âœ… Controle de Qualidade do CÃ³digo

Para garantir a qualidade do cÃ³digo, sÃ£o utilizadas ferramentas como **Pylint** (linting) e **Black** (formataÃ§Ã£o automÃ¡tica). O projeto tambÃ©m utiliza **integraÃ§Ã£o contÃ­nua (CI)** com **GitHub Actions**, assegurando que o cÃ³digo submetido passe por testes automatizados.

## ğŸ› ï¸ InstruÃ§Ãµes de Uso

### 1. **Clonando o RepositÃ³rio**

Primeiro, faÃ§a o clone deste repositÃ³rio para sua mÃ¡quina local:

```bash
git clone https://github.com/GeorgeAlexsander/AutismDetectWithML.git
cd AutismDetectWithML
```

### 2. **Criando e Ativando um Ambiente Virtual**

Para garantir que as dependÃªncias do projeto nÃ£o entrem em conflito com outras bibliotecas instaladas globalmente em seu sistema, Ã© recomendado o uso de um ambiente virtual.

### No Windows

- Crie o ambiente virtual:

```bash
py -m venv venv
```

### No macOS/Linux

- Ative o ambiente virtual:

```bash
source venv/bin/activate
```

---

### 3. Instalando DependÃªncias

Com o ambiente virtual ativo, vocÃª pode instalar todas as dependÃªncias do projeto diretamente a partir do arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

Esse comando garantirÃ¡ que todas as bibliotecas necessÃ¡rias para rodar o projeto e gerar a documentaÃ§Ã£o estejam instaladas.

**Importante**: O Sphinx precisa conseguir acessar e executar o cÃ³digo que estÃ¡ na pasta do projeto. No projeto temos os cÃ³digos-fonte organizados em uma subpasta, como `src/`, entÃ£o Ã© necessÃ¡rio realizar a instalaÃ§Ã£o das dependÃªncias tambÃ©m na pasta onde o cÃ³digo fonte se encontra. Isso Ã© crucial porque o Sphinx utiliza a extensÃ£o `autodoc`, que permite a geraÃ§Ã£o automÃ¡tica de documentaÃ§Ã£o a partir das docstrings do cÃ³digo. Para que o `autodoc` funcione corretamente, ele precisa acessar os mÃ³dulos e suas respectivas dependÃªncias, garantindo assim que a documentaÃ§Ã£o gerada reflita com precisÃ£o o funcionamento do cÃ³digo.

---

### 4. Gerando a DocumentaÃ§Ã£o

A documentaÃ§Ã£o do projeto Ã© gerada utilizando o **Sphinx**. Certifique-se de que o ambiente virtual estÃ¡ ativo e que todas as dependÃªncias foram instaladas.

- Navegue atÃ© o diretÃ³rio da documentaÃ§Ã£o:

```bash
cd docs
```

- Para gerar a documentaÃ§Ã£o em HTML, execute o seguinte comando:

```bash
make html
```

- Caso haja a criaÃ§Ã£o de novos arquivos de cÃ³digo a serem documentados, execute o comando abaixo para gerar automaticamente a documentaÃ§Ã£o dos novos mÃ³dulos, execute na pasta docs o comando a seguir:

```bash
sphinx-apidoc -o . ../src
```

Sendo que o ultimo termo, Ã© o Path para os cÃ³digos-fontes, no exemplo eles estÃ£o na pasta src.

A documentaÃ§Ã£o serÃ¡ gerada no diretÃ³rio `docs/_build/html`. Abra o arquivo `index.html` em um navegador para visualizar a documentaÃ§Ã£o.
