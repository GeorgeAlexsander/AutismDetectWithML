# ğŸ§  Projeto: DetecÃ§Ã£o Precoce de Autismo com Machine Learning

Este projeto visa desenvolver um sistema baseado em **aprendizado de mÃ¡quina** para a detecÃ§Ã£o precoce do autismo por meio da anÃ¡lise de **imagens faciais**. O sistema utiliza **mediÃ§Ãµes antropomÃ©tricas** obtidas a partir de **landmarks faciais**, extraÃ­dos atravÃ©s de tÃ©cnicas de processamento de imagem. A estrutura do projeto estÃ¡ organizada conforme descrito abaixo.

## ğŸ“‚ Estrutura do Projeto

```bash
project_root/
â”‚
â”œâ”€â”€ data/                          # DiretÃ³rio para armazenar dados
â”‚   â”œâ”€â”€ raw/                       # Dados brutos (nÃ£o processados)
â”‚   â”‚   â”œâ”€â”€ autism_dataset/        # Conjunto de dados sobre autismo
â”‚   â”‚   â”‚   â”œâ”€â”€ no_autism/         # Dados de indivÃ­duos sem autismo
â”‚   â”‚   â”‚   â”œâ”€â”€ with_autism/       # Dados de indivÃ­duos com autismo
â”‚   â”‚   â”‚   â””â”€â”€ ...                # Outros subdiretÃ³rios conforme necessÃ¡rio
â”‚   â”‚   â””â”€â”€ processed/             # Dados processados (prontos para uso)
â”‚   â”‚       â””â”€â”€ ...                # Estrutura dos dados processados
â”‚   â””â”€â”€ README.md                  # DocumentaÃ§Ã£o sobre os dados
â”‚
â”œâ”€â”€ notebooks/                     # Notebooks Jupyter para anÃ¡lise
â”‚   â””â”€â”€ exploratory_analysis.ipynb  # AnÃ¡lise exploratÃ³ria dos dados
â”‚
â”œâ”€â”€ src/                           # CÃ³digo-fonte do projeto
â”‚   â”œâ”€â”€ __init__.py                # Torna o diretÃ³rio um pacote Python
â”‚   â”œâ”€â”€ data_processing.py         # Processamento de dados brutos
â”‚   â”œâ”€â”€ feature_extraction.py      # ExtraÃ§Ã£o de caracterÃ­sticas
â”‚   â”œâ”€â”€ model_training.py          # Treinamento do modelo
â”‚   â”œâ”€â”€ model_evaluation.py        # AvaliaÃ§Ã£o do modelo
â”‚   â””â”€â”€ utils.py                   # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ tests/                         # Testes automatizados
â”‚   â”œâ”€â”€ __init__.py                # Torna o diretÃ³rio um pacote Python
â”‚   â”œâ”€â”€ test_data_processing.py    # Testes de processamento de dados
â”‚   â”œâ”€â”€ test_feature_extraction.py # Testes de extraÃ§Ã£o de caracterÃ­sticas
â”‚   â””â”€â”€ ...                        # Outros testes conforme necessÃ¡rio
â”‚
â”œâ”€â”€ docs/                          # DocumentaÃ§Ã£o do projeto
â”‚   â”œâ”€â”€ index.md                   # Ãndice da documentaÃ§Ã£o
â”‚   â””â”€â”€ usage.md                   # Como usar o projeto
â”‚
â”œâ”€â”€ .gitignore                     # Arquivos a serem ignorados pelo Git
â”œâ”€â”€ README.md                      # DocumentaÃ§Ã£o principal do projeto
â”œâ”€â”€ requirements.txt               # DependÃªncias do projeto
â””â”€â”€ setup.py                       # ConfiguraÃ§Ã£o do pacote
```

# ğŸ§  Topologia do Modelo de Machine Learning

## âš™ï¸ Estrutura do Modelo
O modelo de detecÃ§Ã£o de autismo em imagens faciais Ã© composto por trÃªs etapas principais:

### ğŸ§© Etapa 1: ExtraÃ§Ã£o de Landmark Facial
A primeira etapa utiliza uma **CNN prÃ©-treinada** para detectar landmarks faciais. SerÃ£o exploradas trÃªs abordagens:

- **Haarcascade com OpenCV (cv2)**: Detecta landmarks faciais com **68 pontos de referÃªncia**.
- **dlib**: Detecta landmarks faciais com **68 pontos de referÃªncia**, usando o dlib.
- **MediaPipe (Google)**: Utiliza um modelo de **malha facial 3D** com **438 pontos de referÃªncia**.

ğŸ”¹ **A saÃ­da serÃ¡ um conjunto de landmarks faciais**, com 68 pontos para Haarcascade e dlib, e 438 pontos para MediaPipe.

### ğŸ§® Etapa 2: CÃ¡lculo de MediÃ§Ãµes AntropomÃ©tricas
Com os landmarks extraÃ­dos, sÃ£o calculadas **8 mediÃ§Ãµes antropomÃ©tricas** usando a fÃ³rmula da **DistÃ¢ncia Euclidiana**. Os resultados sÃ£o armazenados em um arquivo CSV.

ğŸ“Š **MediÃ§Ãµes**:

| ReferÃªncia 1           | ReferÃªncia 2           | DistÃ¢ncia  | DefiniÃ§Ã£o                |
|------------------------|------------------------|------------|--------------------------|
| Trichion (tr)           | Glabella (gl)          | Vertical   | Altura facial superior    |
| Glabella (gl)           | Filtrum superior (pu)  | Vertical   | Altura facial mÃ©dia       |
| Parte superior (pu)     | Menton (me)            | Vertical   | Altura facial inferior    |
| Filtrum superior (pu)   | Filtrum inferior (pl)  | Vertical   | Filtrum                  |
| Endo canthus (enl)      | Endo canthus (enr)     | Horizontal | Largura intercantal       |
| Exo canthus (exl)       | Exo canthus (exr)      | Horizontal | Largura biocular          |
| Alare (all)             | Alare (alr)            | Horizontal | Largura nasal             |
| Cheilion (chr)          | Cheilion (chl)         | Horizontal | Largura da boca           |

### ğŸ§‘â€ğŸ’» Etapa 3: ClassificaÃ§Ã£o com Redes Neurais e Algoritmos de Machine Learning
Os dados extraÃ­dos (CSV com mediÃ§Ãµes antropomÃ©tricas) serÃ£o utilizados para treinar um modelo de classificaÃ§Ã£o. SerÃ£o experimentados trÃªs algoritmos:

- **CNN (Redes Neurais Convolutivas)**: Para classificaÃ§Ã£o baseada em mediÃ§Ãµes antropomÃ©tricas.
- **K-Nearest Neighbors (KNN)**: Para comparaÃ§Ã£o de precisÃ£o.
- **Random Forest**: Para comparaÃ§Ã£o de desempenho e precisÃ£o.

ğŸ“ **O objetivo Ã© prever se a crianÃ§a apresenta sinais de autismo.**

## ğŸ“‹ PadronizaÃ§Ã£o do RepositÃ³rio e Ferramentas Utilizadas

### ğŸ—‚ï¸ Estrutura e ConfiguraÃ§Ã£o do RepositÃ³rio
A estrutura do repositÃ³rio segue um padrÃ£o para facilitar a organizaÃ§Ã£o e manutenÃ§Ã£o do projeto. Ã‰ utilizado o sistema de controle de versÃ£o Git com o modelo de branches Git Flow.

ğŸ”§ **OrganizaÃ§Ã£o do repositÃ³rio** inclui as pastas descritas acima, com detalhes a seguir:

- **data/**: Armazena dados brutos e processados.
- **notebooks/**: Armazena Jupyter Notebooks para anÃ¡lise exploratÃ³ria.
- **src/**: ContÃ©m o cÃ³digo-fonte do projeto, dividido em mÃ³dulos.
- **tests/**: Armazena testes automatizados.
- **docs/**: DocumentaÃ§Ã£o do projeto.

### ğŸ› ï¸ Controle de VersÃ£o e ColaboraÃ§Ã£o
Utilize branches para funcionalidades (**feature/**), correÃ§Ãµes (**bugfix/**), e versÃµes (**release/**), garantindo organizaÃ§Ã£o e rastreabilidade.

### ğŸ§‘â€ğŸ’» ConfiguraÃ§Ã£o do Ambiente de Desenvolvimento
O ambiente de desenvolvimento utiliza o **Visual Studio Code** com extensÃµes para **Python**, **Git**, e **Docker**. Ferramentas adicionais incluem:

- **Pylint**: Para linting.
- **Black**: Para formataÃ§Ã£o automÃ¡tica de cÃ³digo.
- **Jupyter Notebooks**: Para experimentaÃ§Ã£o e anÃ¡lise de dados.

### ğŸ”§ Linguagem de ProgramaÃ§Ã£o e Bibliotecas
O sistema Ã© desenvolvido em **Python**, utilizando bibliotecas como:

- **TensorFlow** e **Keras**: Para desenvolvimento e treinamento do modelo.
- **OpenCV**: Para processamento de imagens.
- **Pandas** e **NumPy**: Para manipulaÃ§Ã£o de dados.

### âœ… Controle de Qualidade do CÃ³digo
Para garantir a qualidade do cÃ³digo, sÃ£o utilizadas ferramentas como **Pylint** (linting) e **Black** (formataÃ§Ã£o automÃ¡tica). O projeto tambÃ©m utiliza **integraÃ§Ã£o contÃ­nua (CI)** com **GitHub Actions**, assegurando que o cÃ³digo submetido passe por testes automatizados.

### ğŸ“„ DocumentaÃ§Ã£o de CÃ³digo
A documentaÃ§Ã£o Ã© gerada com o **Sphinx** a partir de docstrings, facilitando a geraÃ§Ã£o automÃ¡tica de documentaÃ§Ã£o tÃ©cnica.
